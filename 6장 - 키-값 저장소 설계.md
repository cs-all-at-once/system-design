# Chapter 6 - 키-값 저장소 설계

## 1. 개요
키-값 저장소(key-value store)는 비관계형(non-relational) 데이터베이스의 일종으로, 각 데이터는 고유 식별자(키)에 연결된 값(value)으로 저장됩니다. 이러한 쌍(pair)은 `put`, `get` 연산으로 저장하고 조회할 수 있습니다.

## 2. 목표
다음과 같은 특성을 지닌 키-값 저장소를 설계합니다.

- 키-값 쌍의 크기는 10KB 이하
- 큰 데이터를 저장할 수 있어야 함
- 높은 가용성 제공
- 자동 확장/축소 지원 (확장성)
- 조정 가능한 데이터 일관성 수준
- 낮은 응답 지연시간(latency)

## 3. 기본 연산
- `put(key, value)` : 키-값 쌍 저장
- `get(key)` : 키에 해당하는 값 조회

## 4. 단일 서버 키-값 저장소
- 장점: 간단하고 빠름 (해시 테이블 기반)
- 단점: 메모리 한계, 단일 장애점(SPOF)

### 개선 방안
- 데이터 압축
- 핫 데이터만 메모리에, 나머지는 디스크로

 
장기적으로 하나의 서버로 운영하는 것은 부족하며 분산으로 저장하는것이 필요하다.

## 5. 분산 키-값 저장소
- 여러 서버에 데이터를 분산 저장 (분산 해시 테이블)
- 핵심 개념: CAP 정리 이해가 필요

### 5.1 CAP 정리

| 특성 | 설명 |
|------|------|
| 일관성(Consistency) | 모든 노드에서 동일한 데이터를 읽을 수 있어야 함 |
| 가용성(Availability) | 항상 응답을 받을 수 있어야 함 |
| 파티션 감내(Partition Tolerance) | 네트워크 장애가 발생해도 시스템이 작동해야 함 |

> CAP 불가능성 정리: 세 가지 특성을 동시에 만족하는 것은 불가능. 실용적으로는 CP 또는 AP를 선택해야 함.

### 5.2 시스템 유형


---

### CAP 이론 기반 시스템 유형 정리

| 시스템 유형 | 보장 특성                                  | 희생하는 특성                       | 사용 예시                    | 적합한 환경                           | 장점             | 단점                    |
| ------ | -------------------------------------- | ----------------------------- | ------------------------ | -------------------------------- | -------------- | --------------------- |
| CA | 일관성 (Consistency) + 가용성 (Availability) | 파티션 감내성 (Partition Tolerance) | 단일 서버 RDBMS 등            | 네트워크 분할이 거의 없는 소규모 시스템           | 일관성과 응답성 모두 보장 | 분산 환경에서는 실현 불가능       |
| CP | 일관성 + 파티션 감내성                          | 가용성                           | HBase, MongoDB (CP 구성) 등 | 금융, 결제 | 데이터 정합성 보장     | 일부 요청이 실패하거나 지연될 수 있음 |
| AP | 가용성 + 파티션 감내성                          | 일관성                           | Cassandra, DynamoDB 등    | SNS, 로그 수집  | 빠른 응답성, 장애에 강함 | 일관성 보장이 어려워 충돌 해결 필요  |

---

### 추가 설명

 CA 시스템은 네트워크 분할이 없다는 가정 하에 일관성과 가용성을 모두 만족할 수 있으나, 실제 분산 시스템에서는 구현이 불가능하다.     
 CP 시스템은 네트워크 분할이 발생할 경우, 일부 노드의 서비스를 중단시키더라도 일관성을 유지하는 데 초점을 맞춘다.    
 AP 시스템은 네트워크 분할 상황에서도 서비스를 계속 제공할 수 있도록 하며, 나중에 일관성을 수렴시킨다 (Eventual Consistency 활용).     


## 6. 시스템 컴포넌트

### 6.1 데이터 파티션 (Sharding)
- 데이터를 키의 해시값 기반으로 여러 서버에 나눠 저장
이때 두가지를 유심히 고려하여야 한다.
1. 데이터를 여러서버에 고르게 분산할 수 있는가?
2. 노드가 추가 혹은 삭제 될때 데이터이동을 최소화 할 수 있는가?

5장에서 배운 **안정해시**가 이러한 문제를 푸는데  적합한 기술이다.
1. 규모 확장 자동화: 시스템 부하에따라 서버가 자동으로 추가 제거 되도록 만들 수 있다.
   > 예: 서버 1개 추가 시, 그 서버와 이웃한 일부 키만 이동
3. 다양성: 각 서버의 용량에 맞게 가상 노드수를 제어할 수 있다. 다시말해, 고성능서버의 경우 더 많은 가상노드를 가지도록 할 수 있다.

### 안정 해시(Consistent Hashing)를 사용하는 이유
안정 해시는 **대규모 분산 시스템에서 서버 추가/제거가 잦거나 서버 성능이 서로 다른 경우**에 매우 유용한 기술이다. 

### 1. 규모 확장 자동화가 가능한 이유
> **Consistent Hashing은 서버의 추가/제거 시 전체 데이터를 재배치하지 않고 일부만 이동**시키는 구조이므로, 서버를 동적으로 조정하는 오토스케일링 환경에서 매우 효율적이다.

* 전체 해시 공간에서 일부 키만 재배치
* 서버 증설/축소 시 다운타임 없이 자동 확장 가능
* 클라우드 환경의 **Auto Scaling** 전략에 적합

### 2. 서버 다양성(이기종 서버 지원)에 적합한 이유

> **가상 노드(Virtual Nodes)를 활용해 각 서버에 할당되는 데이터의 양을 조절**할 수 있다. 이를 통해 고성능 서버는 더 많은 데이터를 처리하고, 저사양 서버는 적은 부하만 담당하게 할 수 있다.

* 가상 노드 수로 서버별 처리량 비율 조절 가능
* 고성능 서버에 더 많은 가상 노드 부여 → **부하 분산 최적화**
* 다양한 스펙의 서버를 조합해 효율적인 자원 운용 가능



### 6.2 데이터 다중화 (Replication)

* 동일한 데이터를 여러 노드에 복제하여 저장함으로써 **장애 발생 시에도 데이터 유실 없이 복구**할 수 있으며, \*\*고가용성(High Availability)\*\*을 확보할 수 있다.
* 일반적으로 복제본 수 `N`을 기준으로 여러 노드를 선택하며, `W`개의 쓰기 성공, `R`개의 읽기 성공 응답을 통해 일관성과 가용성 균형을 조절한다.
* Consistent Hashing과 가상 노드(Virtual Node)를 사용할 경우, 선택된 `N`개의 노드가 **동일한 물리 서버에 중복될 수 있으므로**, 반드시 **서로 다른 물리 서버**를 선택해야 한다.


## 6.3 데이터 일관성

* **강한 일관성 (Strong Consistency)**
  모든 읽기 요청이 최신 쓰기 결과를 반영한다. 즉, 데이터가 항상 동기화되어 있어 즉시 일관된 값을 읽을 수 있다.

* **최종적 일관성 (Eventual Consistency)**
  일시적으로 데이터 불일치가 발생할 수 있으나 시간이 지나면 모든 노드가 동일한 데이터 상태로 수렴한다. 고가용성 환경에서 주로 사용된다.

* **정족수 합의 프로토콜 (Quorum Consensus Protocol)**
  데이터의 일관성과 가용성 간 균형을 맞추기 위해 쓰기와 읽기에 대해 최소 성공 노드 수를 설정한다.

  * 보통 쓰기 성공 수(W)와 읽기 성공 수(R)의 합이 복제본 수(N)를 초과하면 강한 일관성을 보장한다 (W + R > N).


---

## 6.4 일관성 모델

일관성 모델은 시스템에서 데이터의 일관성과 가용성 사이에 어떤 균형을 맞출지 결정한다.
이를 위해 읽기와 쓰기 연산에 필요한 정족수(쿼럼)인 **W**(쓰기), **R**(읽기), 그리고 전체 복제본 수 **N**을 조절한다.

* **W (Write Quorum)**: 쓰기 작업이 성공하기 위해 반드시 응답을 받아야 하는 서버 수
* **R (Read Quorum)**: 읽기 작업이 성공하기 위해 반드시 응답을 받아야 하는 서버 수
* **N (Replication Factor)**: 데이터가 복제된 전체 서버 수

### 일관성과 가용성의 균형 조절

* **W와 R 값을 작게 설정할 경우**

  * 읽기/쓰기 응답 속도가 빨라지고 가용성이 높아진다.
  * 하지만 데이터 일관성이 약해질 수 있다. (최신 데이터가 아닐 가능성 존재)
* **W와 R 값을 크게 설정할 경우**

  * 강한 일관성이 보장된다.
  * 그러나 응답 지연이 늘어나고, 일부 서버가 다운되면 가용성이 떨어질 수 있다.

### 예시

* **W=1, R=1, N=3**

  * 읽기와 쓰기 작업 모두 최소 1개의 서버 응답만 받아도 성공으로 간주한다.
  * 가용성은 높지만 일관성은 낮아, 최신 데이터가 보장되지 않을 수 있다.
* **강한 일관성 조건**

  * W + R > N일 때, 항상 읽기와 쓰기 사이에 최소 한 개의 최신 데이터를 가진 서버가 포함된다.
  * 이 경우 최신 데이터가 보장된다.

---

## 6.5 비일관성 해소기법: 데이터 버저닝

복제된 데이터는 동시 다발적으로 변경될 수 있어, 버전 충돌이 발생할 수 있다.
이러한 비일관성을 해소하기 위해 **데이터 버저닝(versioning)** 기법을 사용한다.

### 벡터 클럭(Vector Clock)

* 각 복제본은 자신이 수정된 이력을 벡터 형태로 기록한다.
* 벡터 클럭은 각 노드가 수정한 횟수와 타임스탬프를 함께 추적한다.
* 이를 통해 어떤 버전이 최신인지, 또는 충돌이 발생했는지 판단 가능하다.

<img width="1186" alt="image" src="https://github.com/user-attachments/assets/92f81abc-1c14-4535-bdce-32011d0fc7dd" />


### 충돌 관리

* 충돌이 발생하면 벡터 클럭을 비교하여 두 버전 간 선후 관계를 판단한다.
* 만약 어느 한쪽이 다른 쪽을 포함(계승)하지 않는다면, 충돌로 간주한다.
* 충돌 시에는 자동 병합 알고리즘을 적용하거나, 사용자 개입을 통해 해결할 수 있다.
  
1. 스케일링 문제 (확장성 한계)
- 벡터 시계는 각 노드마다 별도의 카운터를 유지하는데, 노드 수가 많아질수록 벡터의 크기가 커진다.
- 이로 인해 네트워크 오버헤드가 증가하고, 저장 공간과 계산 비용이 커지는 문제가 발생한다.

2. 복잡성 증가
- 벡터 시계를 사용하면 버전 간 인과관계를 비교하는 연산이 복잡해진다.
- 충돌이 발생했을 때, 여러 버전을 병합하거나 사용자 개입이 필요한 경우가 많아 구현과 운영이 어려워질 수 있다.


### 6.6 장애 처리

#### 분산형 장애 감지: 가십 프로토콜(Gossip Protocol)

멀티캐스팅 방식보다는 보통 가십 프로토콜과 같은 분산형 장애 감지 솔루션을 사용하는 편이 보다 효율적이다.

**가십 프로토콜의 동작 원리**

1. 각 노드는 멤버십 목록을 유지한다. 멤버십 목록은 각 멤버 ID와 박동 카운터(Heartbeat Counter)의 쌍으로 구성된다.
2. 각 노드는 주기적으로 자신의 박동 카운터를 1씩 증가시킨다.
3. 각 노드는 무작위로 선택된 다른 노드들에게 자신의 박동 카운터 목록을 주기적으로 전송한다.
4. 박동 카운터 목록을 받은 노드는 멤버십 목록을 최신 값으로 갱신한다.
5. 만약 어떤 멤버의 박동 카운터가 일정 시간 동안 갱신되지 않으면, 해당 멤버를 장애(오프라인) 상태로 간주한다.

이 방식은 중앙 집중형 방식보다 확장성이 뛰어나며, 네트워크 부하를 분산시켜 장애 감지의 효율성과 신뢰성을 높인다.


### 일시적 장애 처리

#### 임시 위탁 (Hinted Handoff)

* **임시 위탁**은 일시적으로 장애가 발생한 노드에 데이터를 바로 쓰지 못할 때, 정상 노드가 대신 임시로 데이터를 저장하는 기법이다.
* 장애 노드가 복구되면, 정상 노드에 임시 저장된 데이터를 장애 노드로 전달해 동기화한다.
* 이 방식은 느슨한 정족수 환경에서 데이터 손실을 줄이고, 가용성을 높이기 위해 사용된다.

영구 장애 처리에 머클트리(Merkle Tree)를 사용하는 내용을 보기 좋게 정리해드릴게요.


### 영구 장애 처리 – 머클트리(Merkle Tree) 활용
반-엔트로피(Anti-Entropy) 프로토콜을 이용한다.       
분산 시스템에서 데이터 복제본 간의 **불일치(비일관성)**를 주기적으로 감지하고 수정하여, 데이터 일관성을 회복하는 데 사용되는 프로토콜이다.
![image](https://github.com/user-attachments/assets/0c7566fa-c558-4552-a096-60f5c816c330)

* **목적**
  영구 장애로 인해 데이터가 손실되거나 일부 노드가 복구된 경우, 데이터 복제본 간의 불일치(차이점)를 효과적으로 찾아내고 동기화하는 데 사용한다.

### 머클트리(Merkle Tree)란?

* 해시 트리 구조로, 데이터 블록들을 해시한 후 이를 다시 해시하여 트리 형태로 만든다.
* 루트 해시값만으로도 전체 데이터의 무결성을 빠르게 검증할 수 있다.

### 머클트리를 사용하는 이유

* **효율적인 데이터 검증**
  장애 복구 시 전체 데이터를 일일이 비교하지 않고 루트 해시를 통해 빠르게 일치 여부 확인 가능.
* **부분 동기화 지원**
  데이터 일부가 불일치할 경우, 머클트리를 이용해 불일치 구간만 빠르게 찾아내어 필요한 데이터만 동기화.
* **네트워크 및 처리 비용 절감**
  전체 데이터 전송 없이 필요한 부분만 전송하여 비용과 시간을 절약.
* **확장성 우수**
  대용량 데이터에서도 빠르고 효율적으로 동기화 가능.



## 분산 키-값 저장소의 데이터 흐름

### 쓰기 경로 (Write Path)

1. **커밋 로그(Commit Log) 기록**

   * 클라이언트가 데이터를 쓰면, 해당 요청은 가장 먼저 커밋 로그에 기록된다.
   * 이는 장애 발생 시 데이터를 복구할 수 있도록 보장하는 지속성(Persistence) 확보를 위한 단계이다.

2. **메모리 캐시에 쓰기 (MemTable Write)**

   * 커밋 로그에 기록한 후, 데이터는 메모리 내의 캐시인 MemTable에 저장된다.
   * MemTable은 휘발성이지만, 메모리 기반이기 때문에 쓰기 속도가 빠르다.

3. **디스크로 플러시 (Flush to SSTable)**

   * MemTable이 가득 차거나 일정 임계치를 초과하면, 그 내용을 디스크에 저장된 SSTable로 옮긴다.
   * SSTable(Sorted String Table)은 정렬된 형태의 <키, 값> 쌍을 보관하는 불변(immutable) 파일이다.
   * 정렬 구조 덕분에 추후 조회 성능이 향상된다.

---

### 읽기 경로 (Read Path)

1. **MemTable 확인**

   * 클라이언트가 읽기 요청을 보내면, 우선 메모리 상의 MemTable에 해당 키가 있는지 확인한다.
   * 있다면, 즉시 값을 반환한다.

2. **블룸 필터 검사 (Bloom Filter Check)**

```plaintext
읽기 요청: key42

1. MemTable에 key42 없음 → 디스크 접근 필요
2. 여러 개의 SSTable이 있을 때
   → 블룸 필터 검사 결과:
      - SSTable1: 없음 (확실)
      - SSTable2: 있음일 수 있음 (거짓 긍정 가능)
      - SSTable3: 없음 (확실)
3. SSTable2만 디스크 조회 → 효율적
```


   * MemTable에 없다면, 디스크에 있는 여러 SSTable 중 어느 곳에 해당 키가 존재할 수 있는지 판단해야 한다.
   * 이때 블룸 필터를 사용한다.
   * 블룸 필터는 "거짓 긍정은 허용하지만 거짓 부정은 없는" 확률적 자료구조로, 특정 SSTable에 키가 있을 가능성을 빠르게 판단할 수 있다.     
(블룸 필터는 어떤 값이 "없다"는 건 확실히 말해주고, "있다"는 건 대충 말해주는 빠르고 가벼운 필터이다.)
3. **SSTable에서 데이터 조회**

   * 블룸 필터가 지목한 SSTable을 조회한다.
   * SSTable은 내부적으로 인덱스를 가지고 있어 빠르게 데이터에 접근할 수 있다.

4. **결과 반환**
   * 조회한 데이터를 클라이언트에게 반환한다.


